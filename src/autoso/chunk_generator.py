"""
AutoSo - Chunk Generator
=======================

A high-performance text chunking system that processes transcript outputs to create overlapping
segments optimized for live speech matching and text analysis. Designed to work seamlessly with
the TranscriptGenerator output, this module provides efficient text segmentation with source tracking.

Key Features:
- Configurable sliding window for flexible chunk sizing
- Parallel processing for handling multiple documents efficiently
- Source tracking for each word in generated chunks
- Comprehensive logging and error handling
- Type-annotated codebase for maintainability
- Optimized for integration with speech processing pipelines

Core Components:
- ChunkGenerator: Central class managing the chunk generation workflow
  - _get_words: Processes transcripts into word-level tokens with metadata
  - _load_transcripts_from_file: Handles JSON transcript loading and validation
  - _process_single_item: Manages end-to-end processing of individual documents
  - generate_chunks: Coordinates parallel processing of document batches

Data Models:
- ChunkInputItem: Configuration for chunk generation tasks
- Output chunks include source tracking and metadata for each segment

Dependencies:
- Core: joblib, orjson, fastnanoid
- Utils: pathlib, typing, logging
- Note: Requires transcript data in the format generated by TranscriptGenerator
"""

import logging
import sys
import unicodedata
from dataclasses import dataclass
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union, Any
import orjson
import fastnanoid
from joblib import Parallel, delayed

# Configure root logger
root_logger = logging.getLogger()
root_logger.setLevel(logging.INFO)

# Create a console handler with our format
console_handler = logging.StreamHandler(sys.stdout)
console_handler.setLevel(logging.INFO)
formatter = logging.Formatter("%(asctime)s - %(name)s - %(levelname)s: %(message)s")
console_handler.setFormatter(formatter)

# Add our handler to the root logger
root_logger.addHandler(console_handler)

# Create our logger
logger = logging.getLogger("ChunkGenerator")


@dataclass
class ChunkInputItem:
    name: str
    input_path_transcript: Path
    output_dir: Path


class ChunkGenerator:
    """A class to generate overlapping text chunks from transcripts using a sliding window."""

    def __init__(self, window_size: int = 7):
        """Initialize the Chunk Generator with configuration.

        Args:
            window_size: Number of words in each chunk (default: 7)
        """
        self.logger = logging.getLogger("ChunkGenerator")
        self.logger.info("ChunkGenerator initialized")
        self.window_size = window_size
        self.PUNCTUATION_TABLE = {
            i: None
            for i in range(sys.maxunicode)
            if unicodedata.category(chr(i)).startswith("P")
        }

    def _normalize_text(self, text: str) -> str:
        """Normalize text by converting to lowercase, normalizing unicode, and removing punctuation.

        Args:
            text: Input text to normalize

        Returns:
            str: Normalized text with consistent formatting
        """
        normalized = unicodedata.normalize("NFC", text.lower())
        normalized = normalized.replace("-", " ").replace("â€”", " ")
        words = normalized.translate(self.PUNCTUATION_TABLE).split()
        return " ".join(words)

    def _get_words(
        self, transcript: Dict[str, Any], item_name: str
    ) -> List[Dict[str, Any]]:
        """Extract words from a transcript with their source information.

        Args:
            transcript: A dictionary containing transcript data with {transcript, transcript_id} keys
            item_name: Name of the item being processed for logging

        Returns:
            List[Dict[str, Any]]: List of word dictionaries with source information

        Raises:
            KeyError: If required transcript data is missing
        """
        self.logger.debug(f"[{item_name}] Extracting words from transcript")
        try:
            words = []
            text = transcript["transcript"]
            transcript_id = transcript["transcript_id"]
            transcript_index = transcript.get("transcript_index", -1)

            # Normalize the text before splitting into words
            normalized_text = self._normalize_text(text)

            for word in normalized_text.split():
                words.append(
                    {
                        "text": word,
                        "source_transcript": transcript_id,
                        "transcript_index": transcript_index,
                    }
                )

            self.logger.debug(
                f"[{item_name}] Successfully extracted {len(words)} words from transcript"
            )
            return words
        except KeyError as e:
            self.logger.error(
                f"[{item_name}] Missing required transcript data: {str(e)}"
            )
            raise

    def _load_transcripts_from_file(
        self, input_path: Union[str, Path], item_name: str
    ) -> List[Dict[str, Any]]:
        """Load transcripts from a JSON file.

        Args:
            input_path: Path to the JSON file containing transcripts
            item_name: Name of the item being processed for logging

        Returns:
            List[Dict[str, Any]]: List of transcript dictionaries

        Raises:
            FileNotFoundError: If the transcript file doesn't exist
            orjson.JSONDecodeError: If the transcript file contains invalid JSON
        """
        self.logger.info(f"[{item_name}] Loading transcripts from {input_path}")
        try:
            with open(input_path, "rb") as f:
                transcripts = orjson.loads(f.read())
            self.logger.info(
                f"[{item_name}] Successfully loaded {len(transcripts)} transcripts from {input_path}"
            )
            return transcripts
        except FileNotFoundError:
            self.logger.error(f"[{item_name}] Transcript file not found: {input_path}")
            raise
        except orjson.JSONDecodeError as e:
            self.logger.error(
                f"[{item_name}] Invalid JSON in transcript file {input_path}: {str(e)}"
            )
            raise
        except Exception as e:
            self.logger.error(
                f"[{item_name}] Error loading transcripts from {input_path}: {str(e)}"
            )
            raise

    def _save_chunks_to_file(
        self,
        chunks: List[Dict[str, Any]],
        output_path: Union[str, Path],
        item_name: str,
    ) -> None:
        """Save chunks to a JSON file.

        Args:
            chunks: List of chunk dictionaries to save
            output_path: Path where to save the chunks
            item_name: Name of the item being processed for logging

        Raises:
            OSError: If there's an error creating the output directory or writing the file
            TypeError: If chunks cannot be serialized to JSON
        """
        self.logger.info(f"[{item_name}] Saving {len(chunks)} chunks to {output_path}")
        try:
            output_path = Path(output_path)
            output_path.parent.mkdir(parents=True, exist_ok=True)

            with open(output_path, "wb") as f:
                f.write(
                    orjson.dumps(
                        chunks, option=orjson.OPT_INDENT_2 | orjson.OPT_NON_STR_KEYS
                    )
                )

            self.logger.info(
                f"[{item_name}] Successfully saved {len(chunks)} chunks to {output_path}"
            )
        except OSError as e:
            self.logger.error(
                f"[{item_name}] Error saving chunks to {output_path}: {str(e)}"
            )
            raise
        except TypeError as e:
            self.logger.error(
                f"[{item_name}] Error serializing chunks to JSON: {str(e)}"
            )
            raise
        except Exception as e:
            self.logger.error(f"[{item_name}] Unexpected error saving chunks: {str(e)}")
            raise

    def _process_single_item(
        self, item: ChunkInputItem
    ) -> Tuple[Optional[Path], Optional[str]]:
        """Process a single transcript item and generate chunks.

        Args:
            item: ChunkInputItem containing input/output paths and item name

        Returns:
            tuple: (output_path, error) - output_path is the path to the generated
                  chunks if successful, None otherwise. error contains any error
                  message if processing failed.
        """
        self.logger.info(f"[{item.name}] Initiating processing")
        try:
            output_path = item.output_dir / f"{item.name}_chunks.json"
            item.output_dir.mkdir(parents=True, exist_ok=True)

            # Load and process transcripts
            transcripts = self._load_transcripts_from_file(
                item.input_path_transcript, item.name
            )
            if not transcripts:
                error_msg = f"No transcripts found in {item.input_path_transcript}"
                self.logger.warning(f"[{item.name}] {error_msg}")
                return None, error_msg

            # Sort transcripts by their index to maintain order
            sorted_transcripts = sorted(
                transcripts, key=lambda x: x.get("transcript_index", 0)
            )

            # Extract all words with their source information
            all_words = []
            for transcript in sorted_transcripts:
                all_words.extend(self._get_words(transcript, item.name))

            chunks = []
            total_words = len(all_words)

            self.logger.info(
                f"[{item.name}] Generating chunks from {total_words} words with window size {self.window_size}"
            )

            # If there are fewer words than the window size, create a single chunk
            if total_words <= self.window_size:
                chunk_text = " ".join(word["text"] for word in all_words)
                source_transcripts = list(
                    set(word["source_transcript"] for word in all_words)
                )

                chunks = [
                    {
                        "chunk_index": 0,
                        "chunk_id": fastnanoid.generate(),
                        "source_transcripts": source_transcripts,
                        "chunk": chunk_text,
                    }
                ]
            else:
                # Generate sliding window chunks
                for i in range(total_words - self.window_size + 1):
                    window = all_words[i : i + self.window_size]
                    chunk_text = " ".join(word["text"] for word in window)

                    # Get unique source transcripts for this chunk and sort by transcript index
                    unique_transcripts = {}
                    for word in window:
                        tid = word["source_transcript"]
                        idx = word["transcript_index"]
                        if (
                            tid not in unique_transcripts
                            or idx < unique_transcripts[tid]["index"]
                        ):
                            unique_transcripts[tid] = {"id": tid, "index": idx}

                    # Sort by transcript index and extract IDs
                    source_transcripts = [
                        t["id"]
                        for t in sorted(
                            unique_transcripts.values(), key=lambda x: x["index"]
                        )
                    ]

                    chunks.append(
                        {
                            "chunk_index": i,
                            "chunk_id": fastnanoid.generate(),
                            "source_transcripts": source_transcripts,
                            "chunk": chunk_text,
                        }
                    )

            # Save chunks to file
            self._save_chunks_to_file(chunks, output_path, item.name)
            return output_path, None

        except Exception as e:
            error_msg = f"Failed to process: {str(e)}"
            self.logger.error(f"[{item.name}] {error_msg}", exc_info=True)
            return None, error_msg

    def _process_with_logging(
        self, item: ChunkInputItem
    ) -> Tuple[Optional[Path], Optional[str]]:
        """Process a single item with error handling and logging.

        Args:
            item: ChunkInputItem to process

        Returns:
            Tuple of (output_path, error)
        """
        try:
            output_path, error = self._process_single_item(item)
            if error:
                self.logger.error(f"[{item.name}] Error processing: {error}")
            else:
                self.logger.info(
                    f"[{item.name}] Successfully processed and saved chunks to {output_path}"
                )
            return output_path, error
        except Exception as e:
            error_msg = f"Unexpected error processing: {str(e)}"
            self.logger.error(f"[{item.name}] {error_msg}", exc_info=True)
            return None, error_msg

    def generate_chunks(self, items: List[ChunkInputItem]) -> None:
        """Generate chunks from a list of transcript items using parallel processing.

        Args:
            items: List of ChunkInputItem objects containing transcript information

        Raises:
            Exception: If any worker encounters an error
        """
        if not items:
            self.logger.warning("No items to process")
            return

        self.logger.info(
            f"Starting parallel processing for {len(items)} items with {len(items)} workers"
        )

        # Process items in parallel with joblib
        results = Parallel(n_jobs=len(items), prefer="threads")(
            delayed(self._process_with_logging)(item) for item in items
        )

        # Track successfully processed items
        successful_items = []
        for result, item in zip(results, items):
            if result is None:
                self.logger.error(
                    f"[{item.name}] Processing failed: No result returned"
                )
                continue

            output_path, error = result
            if not error:
                successful_items.append(item.name)

        if successful_items:
            self.logger.info(
                f"All items successfully processed: {', '.join(successful_items)}"
            )
        else:
            self.logger.warning("No items were successfully processed")


if __name__ == "__main__":
    # Example file paths
    file_paths = [
        ChunkInputItem(
            name="yasin",
            input_path_transcript=Path("data/yasin/yasin_transcript.json"),
            output_dir=Path("data/yasin"),
        ),
        # ChunkInputItem(
        #     name="ezgi",
        #     input_path_transcript=Path("data/ezgi/ezgi_transcript.json"),
        #     output_dir=Path("data/ezgi"),
        # ),
        # ChunkInputItem(
        #     name="nehir",
        #     input_path_transcript=Path("data/nehir/nehir_transcript.json"),
        #     output_dir=Path("data/nehir"),
        # ),
        # ChunkInputItem(
        #     name="betul",
        #     input_path_transcript=Path("data/betul/betul_transcript.json"),
        #     output_dir=Path("data/betul"),
        # ),
        # ChunkInputItem(
        #     name="hilal",
        #     input_path_transcript=Path("data/hilal/hilal_transcript.json"),
        #     output_dir=Path("data/hilal"),
        # ),
    ]

    # Initialize and run the generator
    generator = ChunkGenerator()
    generator.generate_chunks(file_paths)
